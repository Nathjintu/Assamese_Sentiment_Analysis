{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4",
        "_uuid": "b34dc51c4c60fc1cc8200129e74e7a025fd0cc42",
        "id": "nPIG5ClO3iQA"
      },
      "source": [
        "\n",
        "Sentiment Analysis is the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442",
        "_uuid": "4efef6a6c3143fbb6bca5903fc1a764bbbb861c4",
        "id": "dCCk1r2b3iQE"
      },
      "source": [
        "Using LSTM to classify Assamese text into positive, negative or neutral.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287",
        "_uuid": "717bb968c36b9325c7d4cae5724a3672e49ff243",
        "id": "Paw1Ncc-3iQF"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "#\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e",
        "_uuid": "9b520acffb5cd85d0e1ada968ad0f12cee33a4b5",
        "id": "MmRfYwWq3iQK"
      },
      "source": [
        "Only keeping the necessary columns."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oo9QdbmEHk0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a",
        "_uuid": "d2bc3bbd2ea3961c49e6673145a0a7226c160e58",
        "id": "sqbgJBcK3iQK"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/06_assamese_sentiment_data.csv')\n",
        "data = data[['text','sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = data['sentiment'].value_counts()\n",
        "\n",
        "print(\"Exact counts of each sentiment in the main CSV file:\")\n",
        "print(sentiment_counts)"
      ],
      "metadata": {
        "id": "fmpknSAgYiGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "68989f3bc936825f4425d2d08467ce17c4a2f092",
        "id": "2yOt7NsI3iQL"
      },
      "source": [
        "Data preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "f6a102c71c8e281450f7e73a5678cc9d0bb99e99",
        "id": "d5Tv5wg-3iQL"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929",
        "_uuid": "ff12d183224670f9c4c96fd24581b9924d4dff20",
        "id": "94naAyoV3iQM"
      },
      "source": [
        "here we define the number of max features as 5000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
        "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
        "id": "0ibLELnC3iQQ"
      },
      "outputs": [],
      "source": [
        "##data = data[data.sentiment != \"Neutral\"]\n",
        "#data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "# removing special chars\n",
        "data['text'] = data['text'].astype(str).apply((lambda x: re.sub('[^\\u0980-\\u09ff\\s]','',x)))\n",
        "#\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076",
        "_uuid": "d0f8b4542106a279f7398db7285ae5e370b2e813",
        "id": "V-gQy7H93iQR"
      },
      "outputs": [],
      "source": [
        "print(data[ data['sentiment'] == 'Positive'].size)\n",
        "print(data[ data['sentiment'] == 'Negative'].size)\n",
        "print(data[ data['sentiment'] == 'Neutral'].size)\n",
        "\n",
        "#for idx,row in data.iterrows():\n",
        " #   row[0] = row[0].replace('rt','')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "890a03c9-316e-4d55-98e1-ba29045eff6c",
        "_uuid": "cfcbefe939b72297019e221ca3f5a283974bffff",
        "id": "UPZcVEcS3iQZ"
      },
      "source": [
        "**1. Up-sample Minority Class**\n",
        "\n",
        "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.\n",
        "There are several heuristics for doing so, but the most common way is to simply resample with replacement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52",
        "id": "rkH4Z7CU3iQZ"
      },
      "outputs": [],
      "source": [
        "# Separate majority and minority classes\n",
        "data_majority = data[data['sentiment'] == \"Neutral\"]\n",
        "data_mid=data[data['sentiment'] == \"Negative\"]\n",
        "data_minority = data[data['sentiment'] == \"Positive\"]\n",
        "\n",
        "bias = data_minority.shape[0]/data_majority.shape[0]\n",
        "# lets split train/test data first then\n",
        "train = pd.concat([data_majority.sample(frac=0.8,random_state=200),data_mid.sample(frac=0.8,random_state=200),\n",
        "         data_minority.sample(frac=0.8,random_state=200)])\n",
        "test = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),data_mid.drop(data_mid.sample(frac=0.8,random_state=200).index),\n",
        "        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n",
        "\n",
        "train = shuffle(train)\n",
        "test = shuffle(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "ce10e582bd894ec271f2587ceb618a9cf8ab03c5",
        "id": "UqvprOmJ3iQZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print('positive data in training:',(train.sentiment == 'Positive').sum())\n",
        "print('negative data in training:',(train.sentiment == 'Negative').sum())\n",
        "print('neutral data in training:',(train.sentiment == 'Neutral').sum())\n",
        "#print('negative data in training:',(train.sentiment == 'Negative').sum())\n",
        "print('negative data in test:',(test.sentiment == 'Negative').sum())\n",
        "print('positive data in test:',(test.sentiment == 'Positive').sum())\n",
        "print('neutral data in test:',(test.sentiment == 'Neutral').sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "384ed509b57715fbc7cce5ad37802a8785603b52",
        "id": "QC-OdntM3iQa"
      },
      "outputs": [],
      "source": [
        "# Separate majority and minority classes in training data for upsampling\n",
        "data_majority = train[train['sentiment'] == 'Neutral']\n",
        "data_mid = train[train['sentiment'] == 'Negative']\n",
        "data_minority = train[train['sentiment'] == 'Positive']\n",
        "\n",
        "print(\"majority class before upsample:\",data_majority.shape)\n",
        "print(\"mid class before upsample:\",data_mid.shape)\n",
        "print(\"minority class before upsample:\",data_minority.shape)\n",
        "\n",
        "# Upsample minority class\n",
        "data_minority_upsampled = resample(data_minority,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "data_mid_upsampled = resample(data_mid,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples= data_majority.shape[0],    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "data_upsampled = pd.concat([data_majority, data_mid_upsampled,data_minority_upsampled])\n",
        "\n",
        "# Display new class counts\n",
        "print(\"After upsampling\\n\",data_upsampled.sentiment.value_counts(),sep = \"\")\n",
        "\n",
        "max_fatures = 5000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "# This is the correct line. It only uses the training data.\n",
        "tokenizer.fit_on_texts(data_upsampled['text'].values)\n",
        "#tokenizer.fit_on_texts(data['text'].values) # training with whole data\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_upsampled['text'].values)\n",
        "X_train = pad_sequences(X_train,maxlen=29)\n",
        "Y_train = pd.get_dummies(data_upsampled['sentiment']).values\n",
        "print('x_train shape:',X_train.shape)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test['text'].values)\n",
        "X_test = pad_sequences(X_test,maxlen=29)\n",
        "Y_test = pd.get_dummies(test['sentiment']).values\n",
        "print(\"x_test shape\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93",
        "_uuid": "05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda",
        "id": "cwW-Y6r-3iQa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# First, we import everything we need for the model, including Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow   .keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 192\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_fatures, embed_dim, input_shape=(X_train.shape[1],)))\n",
        "#model.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\n",
        "\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(Bidirectional(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.0)))\n",
        "#model.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.0))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#model.build(input_shape=(None, X.shape[1]))\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6",
        "_uuid": "8799a667a2c0254cb367c193d86e07ee36d91dd7",
        "id": "hLvz5mqe3iQa"
      },
      "source": [
        "Here we train the Network. We should run much more than 5 epoch, but I would have to wait forever for kaggle, so it is 5 for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b",
        "_uuid": "d0b239912cf67294a9f5af6883bb159c44318fc7",
        "id": "aQrVQ-QG3iQb",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "# also adding weights\n",
        "\"\"\"class_weights = {0: 1.6/bias ,\n",
        "                1: 1,\n",
        "                 2:2}\"\"\"\n",
        "# This is the correct line\n",
        "model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2803c5699e0aec22463aadccd1255e63155c1b09",
        "id": "VNnR9ZGq3iQb"
      },
      "outputs": [],
      "source": [
        "Y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
        "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
        "print(classification_report(df_test.true, df_test.pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7c23e4d6b4cab5b46a58b4c2962abd7a0d29df42",
        "id": "lVUPcg5V3iQb"
      },
      "source": [
        "So the class imbalance is reduced significantly recall values for some classes improved. It is alwayes not possible to reduce it compleatly.\n",
        "\n",
        "You may also noticed some mismatched of the recall values, sometimes increasing, sometimes decreasing.  This can be improved using training model to more epocs and tuning the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "085294a5409b58c2188019177041ceb1756f1826",
        "id": "6PdYCrf83iQb"
      },
      "outputs": [],
      "source": [
        "# running model to few more epochs\n",
        "model.fit(X_train, Y_train, epochs = 30, batch_size=batch_size, verbose = 1,)\n",
        "\n",
        "Y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\n",
        "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\n",
        "print(classification_report(df_test.true, df_test.pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd",
        "_uuid": "d9aac68e2013b3beffb6a764cc5b85be83073e66",
        "id": "ZhGZIiXG3iQc"
      },
      "outputs": [],
      "source": [
        "twt = ['মাংসখিনি বৰ সুস্বাদু হৈছিল']\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences(twt)\n",
        "print(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=29, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")\n",
        "elif (np.argmax(sentiment) == 2):\n",
        "    print(\"neutral\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('assamese_sentiment_model.keras')\n",
        "print(\"Model has been saved successfully as 'assamese_sentiment_model.keras'\")"
      ],
      "metadata": {
        "id": "hXwa_IngEFm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}